{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPel+M/zwuqrJQi24LmH0l/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k-ferry/cs676-fall-2025/blob/main/project-1/deliverable3/deliverable3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"PERPLEXITY_API_KEY\"] = userdata.get(\"PERPLEXITY_API_KEY\")  # sets it for src/pplx_client.py\n"
      ],
      "metadata": {
        "id": "EO8_M0KlaakL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"src\", exist_ok=True)  # create src/ if it doesn't exist\n"
      ],
      "metadata": {
        "id": "2NF8sk05Ys4a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B8ReStXRtlb",
        "outputId": "626d717f-d07b-41ab-a96a-3ca0bbc40b64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/scorer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/scorer.py\n",
        "# scorer.py\n",
        "# Hybrid, interpretable credibility scorer specialized for soccer-card sources.\n",
        "# This version:\n",
        "#   - Removes irrelevant .gov/.edu priors\n",
        "#   - Adds host-specific priors for eBay/COMC/PWCC/Goldin/etc. + manufacturers/graders\n",
        "#   - Expands hobby terms (e.g., Sapphire, Logofractor, 1/1)\n",
        "#   - Keeps weights modest so content/seller evidence still drives scores\n",
        "#\n",
        "# Public contract (stable):\n",
        "#   score_url(url) -> {\n",
        "#     \"url\": str,\n",
        "#     \"status\": \"ok\" | \"invalid_url\" | \"fetch_error\" | ...,\n",
        "#     \"score\": {\"absolute\": float, \"percentile\": float|None},\n",
        "#     \"signals\": [{\"name\",\"value\",\"weight\",\"rationale\"}, ...],\n",
        "#     \"errors\": [str, ...],\n",
        "#     \"meta\": {\"host\": str, \"is_ebay\": bool, \"fetched_at\": iso, \"elapsed_ms\": int, \"fetch_ms\": int|None, \"version\": str}\n",
        "#   }\n",
        "\n",
        "from __future__ import annotations\n",
        "import dataclasses\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import typing as t\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime, timezone\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Optional deps (graceful fallbacks)\n",
        "try:\n",
        "    from bs4 import BeautifulSoup  # type: ignore\n",
        "except Exception:\n",
        "    BeautifulSoup = None\n",
        "\n",
        "try:\n",
        "    import requests\n",
        "except Exception:\n",
        "    requests = None\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "except Exception:\n",
        "    pd = None\n",
        "\n",
        "# ----------------------------\n",
        "# Network defaults (fetch path)\n",
        "# ----------------------------\n",
        "DEFAULT_TIMEOUT_S = 6.0\n",
        "DEFAULT_HEADERS = {\n",
        "    \"User-Agent\": \"CredScorer/0.2 (+https://example.edu/project)\",\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "}\n",
        "\n",
        "# ---------------------------------\n",
        "# Domain recognizers (compiled once)\n",
        "# ---------------------------------\n",
        "# Keep EbayLike for fast checks elsewhere\n",
        "EbayLike = re.compile(r\"(^|\\.)ebay\\.(com|co\\.[a-z]{2}|[a-z]{2})$\", re.I)\n",
        "\n",
        "MarketplacePatterns = {\n",
        "    # Marketplaces\n",
        "    \"ebay\":        re.compile(r\"(^|\\.)ebay\\.(com|co\\.[a-z]{2}|[a-z]{2})$\", re.I),\n",
        "    \"comc\":        re.compile(r\"(^|\\.)comc\\.com$\", re.I),\n",
        "    \"pwcc\":        re.compile(r\"(^|\\.)pwccmarketplace\\.com$\", re.I),\n",
        "    \"goldin\":      re.compile(r\"(^|\\.)goldin\\.co$\", re.I),\n",
        "    \"myslabs\":     re.compile(r\"(^|\\.)myslabs\\.com$\", re.I),\n",
        "    \"alt\":         re.compile(r\"(^|\\.)alt\\.xyz$\", re.I),\n",
        "    \"stockx\":      re.compile(r\"(^|\\.)stockx\\.com$\", re.I),\n",
        "    \"whatnot\":     re.compile(r\"(^|\\.)whatnot\\.com$\", re.I),\n",
        "\n",
        "    # Manufacturers\n",
        "    \"topps\":       re.compile(r\"(^|\\.)topps\\.com$\", re.I),\n",
        "    \"panini\":      re.compile(r\"(^|\\.)paniniamerica\\.net$\", re.I),\n",
        "    \"upperdeck\":   re.compile(r\"(^|\\.)upperdeck\\.com$\", re.I),\n",
        "\n",
        "    # Grading / population / cert lookup\n",
        "    \"psa\":         re.compile(r\"(^|\\.)psacard\\.com$\", re.I),\n",
        "    \"beckett\":     re.compile(r\"(^|\\.)beckett\\.com$\", re.I),\n",
        "    \"sgc\":         re.compile(r\"(^|\\.)gosgc\\.com|(^|\\.)sgccard\\.com$\", re.I),\n",
        "\n",
        "    # Hobby references (light positive)\n",
        "    \"tcdb\":        re.compile(r\"(^|\\.)tradingcarddb\\.com$\", re.I),\n",
        "    \"beckett_pg\":  re.compile(r\"(^|\\.)beckett\\.com/(price-guide|news)\", re.I),\n",
        "    \"130point\":    re.compile(r\"(^|\\.)130point\\.com$\", re.I),\n",
        "}\n",
        "\n",
        "LowSignalPatterns = {\n",
        "    \"pinterest\":   re.compile(r\"(^|\\.)pinterest\\.\", re.I),\n",
        "    \"medium\":      re.compile(r\"(^|\\.)medium\\.com$\", re.I),\n",
        "    \"blogspot\":    re.compile(r\"(^|\\.)blogspot\\.\", re.I),\n",
        "    \"tiktok\":      re.compile(r\"(^|\\.)tiktok\\.com$\", re.I),\n",
        "    \"shorteners\":  re.compile(r\"(^|\\.)bit\\.ly$|(^|\\.)tinyurl\\.com$|(^|\\.)t\\.co$\", re.I),\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# Data classes & contract\n",
        "# -----------------------\n",
        "@dataclass\n",
        "class Signal:\n",
        "    \"\"\"Interpretable scoring component (normalized value & modest weight).\"\"\"\n",
        "    name: str\n",
        "    value: float     # 0..1 (higher is better)\n",
        "    weight: float    # 0..1 (influence)\n",
        "    rationale: str\n",
        "    def contribution(self) -> float:\n",
        "        return self.value * self.weight\n",
        "\n",
        "@dataclass\n",
        "class ScoreResult:\n",
        "    url: str\n",
        "    status: str\n",
        "    score_abs: float\n",
        "    score_pct: float | None\n",
        "    signals: list[Signal]\n",
        "    errors: list[str]\n",
        "    meta: dict[str, t.Any]\n",
        "\n",
        "def response_json(result: ScoreResult) -> dict:\n",
        "    return {\n",
        "        \"url\": result.url,\n",
        "        \"status\": result.status,\n",
        "        \"score\": {\"absolute\": result.score_abs, \"percentile\": result.score_pct},\n",
        "        \"signals\": [dataclasses.asdict(s) for s in result.signals],\n",
        "        \"errors\": result.errors,\n",
        "        \"meta\": result.meta,\n",
        "    }\n",
        "\n",
        "# -------------\n",
        "# Small helpers\n",
        "# -------------\n",
        "def _cheap_text(html: str) -> str:\n",
        "    \"\"\"HTML→text, prefer BeautifulSoup, fallback to regex.\"\"\"\n",
        "    if 'BeautifulSoup' in globals() and BeautifulSoup is not None:\n",
        "        try:\n",
        "            soup = BeautifulSoup(html, \"lxml\")\n",
        "        except Exception:\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "        for tag in soup([\"script\",\"style\",\"noscript\"]):\n",
        "            tag.decompose()\n",
        "        text = soup.get_text(\" \", strip=True)\n",
        "        return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # Fallback\n",
        "    text = re.sub(r\"<script[\\s\\S]*?</script>\", \" \", html, flags=re.I)\n",
        "    text = re.sub(r\"<style[\\s\\S]*?</style>\", \" \", text, flags=re.I)\n",
        "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "def _count_images(html: str) -> int:\n",
        "    if 'BeautifulSoup' in globals() and BeautifulSoup is not None:\n",
        "        try:\n",
        "            soup = BeautifulSoup(html, \"lxml\")\n",
        "        except Exception:\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "        return len(soup.find_all(\"img\"))\n",
        "    return len(re.findall(r\"<img\\b\", html, re.I))\n",
        "\n",
        "def _squash_0_100(raw: float) -> float:\n",
        "    \"\"\"Smooth logistic mapping from raw contribution sum → 0..100 (avoid false precision).\"\"\"\n",
        "    x = raw - 0.8\n",
        "    sig = 1 / (1 + math.exp(-3.5 * x))\n",
        "    return round(100 * sig, 2)\n",
        "\n",
        "def _percentile(x: float, arr: list[float]) -> float:\n",
        "    if not arr:\n",
        "        return float(\"nan\")\n",
        "    rank = sum(1 for a in arr if a <= x)\n",
        "    return round(100 * rank / len(arr), 2)\n",
        "\n",
        "def _now_iso() -> str:\n",
        "    return datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "def _elapsed_ms(t0: float) -> int:\n",
        "    return int((time.perf_counter() - t0) * 1000)\n",
        "\n",
        "def _synthetic_page_for(host: str) -> str:\n",
        "    \"\"\"Deterministic synthetic HTML for dry_run=True (stable tests).\"\"\"\n",
        "    if EbayLike.search(host or \"\"):\n",
        "        return (\n",
        "            \"<html><head><title>eBay Listing</title></head><body>\"\n",
        "            \"Top Rated Seller (99.7% positive feedback) (12450) feedback. \"\n",
        "            \"2024 Topps Chrome UEFA Refractor PSA 10 Rookie /99 auto. \"\n",
        "            \"Ships from New York. 30 day returns. <img/><img/><img/><img/><img/><img/>\"\n",
        "            \"</body></html>\"\n",
        "        )\n",
        "    return (\n",
        "        \"<html><body>By John Doe. Published 2023. References: https://doi.org/10.x/y \"\n",
        "        \"This is a sample article body with some length and structure.</body></html>\"\n",
        "    )\n",
        "\n",
        "# -------------------------\n",
        "# Host prior (domain score)\n",
        "# -------------------------\n",
        "def _host_category(host: str) -> tuple[str, float, float, str]:\n",
        "    \"\"\"\n",
        "    Map host -> (category, value [0..1], weight [0..1], rationale).\n",
        "    Priors are modest; evidence from content/seller specifics dominates.\n",
        "    \"\"\"\n",
        "    h = (host or \"\").lower()\n",
        "\n",
        "    # Marketplaces (ordered by vetting strength / typical buyer protection)\n",
        "    for name, pat in MarketplacePatterns.items():\n",
        "        if pat.search(h):\n",
        "            if name == \"ebay\":\n",
        "                return (\"marketplace\", 0.78, 0.12, \"Trusted marketplace (eBay)\")\n",
        "            if name in {\"pwcc\", \"goldin\"}:\n",
        "                return (\"marketplace\", 0.76, 0.11, f\"Reputable auction marketplace ({name})\")\n",
        "            if name in {\"comc\", \"myslabs\", \"alt\"}:\n",
        "                return (\"marketplace\", 0.72, 0.10, f\"Known card marketplace ({name})\")\n",
        "            if name in {\"stockx\", \"whatnot\"}:\n",
        "                return (\"marketplace\", 0.66, 0.08, f\"General marketplace ({name})\")\n",
        "\n",
        "    # Authorities\n",
        "    for name in [\"topps\", \"panini\", \"upperdeck\", \"psa\", \"beckett\", \"sgc\"]:\n",
        "        if MarketplacePatterns[name].search(h):\n",
        "            return (\"authority\", 0.80, 0.12, f\"Official/authority ({name})\")\n",
        "\n",
        "    # Hobby references (light)\n",
        "    for name in [\"tcdb\", \"beckett_pg\", \"130point\"]:\n",
        "        if MarketplacePatterns[name].search(h):\n",
        "            return (\"reference\", 0.68, 0.08, f\"Hobby reference ({name})\")\n",
        "\n",
        "    # Low-signal hosts (light negative)\n",
        "    for name, pat in LowSignalPatterns.items():\n",
        "        if pat.search(h):\n",
        "            return (\"low_signal\", 0.50, 0.05, f\"Low-signal host ({name})\")\n",
        "\n",
        "    # Generic baselines\n",
        "    if h.endswith(\".com\"):\n",
        "        return (\"generic\", 0.60, 0.06, \".com baseline\")\n",
        "    return (\"unknown\", 0.52, 0.05, \"Unknown/low-signal domain\")\n",
        "\n",
        "def _signal_domain_baseline(host: str) -> Signal:\n",
        "    category, val, wt, why = _host_category(host)\n",
        "    return Signal(\"domain_prior\", val, wt, why)\n",
        "\n",
        "def _signal_transport_security(scheme: str) -> Signal:\n",
        "    return Signal(\"https\", 1.0 if scheme == \"https\" else 0.4, 0.04, \"HTTPS vs HTTP transport\")\n",
        "\n",
        "# ----------------------------\n",
        "# Content heuristics (generic)\n",
        "# ----------------------------\n",
        "def _signals_content_quality(html: str) -> list[Signal]:\n",
        "    s: list[Signal] = []\n",
        "    text = _cheap_text(html)\n",
        "    n = len(text.split())\n",
        "\n",
        "    # Length band (very short is suspect; very long = diminishing returns)\n",
        "    if n <= 30:\n",
        "        v, why = 0.20, \"Very short body\"\n",
        "    elif n <= 120:\n",
        "        v, why = 0.55, \"Short body\"\n",
        "    elif n <= 2500:\n",
        "        v, why = 0.80, \"Reasonable body length\"\n",
        "    else:\n",
        "        v, why = 0.60, \"Very long body\"\n",
        "    s.append(Signal(\"content_length\", v, 0.07, why))\n",
        "\n",
        "    # Outbound refs/links density (weak proxy for sourcing)\n",
        "    cites = len(re.findall(r\"(doi\\.org/|https?://)\\S+\", text))\n",
        "    s.append(Signal(\"citations_links\", min(cites/5, 1.0), 0.04, \"Outbound refs/links density\"))\n",
        "\n",
        "    # Author/date hint\n",
        "    has_authorish = bool(re.search(r\"\\bby\\s+[A-Z][a-z]+\", text))\n",
        "    s.append(Signal(\"author_block_hint\", 1.0 if has_authorish else 0.5, 0.03, \"Author/date block hints\"))\n",
        "    return s\n",
        "\n",
        "# ---------------------------------------\n",
        "# eBay-aware signals + lightweight sentiment\n",
        "# ---------------------------------------\n",
        "CARD_TERMS = {\n",
        "    # Rookie / desirability\n",
        "    \"rookie\": 0.12, \"rc\": 0.08, \"rookie card\": 0.10, \"true rookie\": 0.08,\n",
        "\n",
        "    # Grading\n",
        "    \"psa 10\": 0.16, \"bgs 9.5\": 0.10, \"sgc 10\": 0.08, \"gem mint\": 0.12,\n",
        "\n",
        "    # Serial/auto\n",
        "    \"1/1\": 0.14, \"one of one\": 0.14, \"auto\": 0.12, \"autograph\": 0.12, \"/\": 0.10,\n",
        "\n",
        "    # Sets/variants (Topps Chrome universe and friends)\n",
        "    \"refractor\": 0.08, \"sapphire\": 0.08, \"logofractor\": 0.10, \"mojo\": 0.06, \"speckle\": 0.06,\n",
        "    \"aqua\": 0.05, \"gold\": 0.06, \"orange\": 0.06, \"red\": 0.06, \"black\": 0.06,\n",
        "    \"prizm\": 0.08, \"topps\": 0.06, \"merlin\": 0.06, \"select\": 0.06, \"optic\": 0.06,\n",
        "    \"megacracks\": 0.10, \"megarcracks\": 0.08,  # common misspelling safety net\n",
        "}\n",
        "\n",
        "_POS = {\"grail\",\"pc\",\"beautiful\",\"clean\",\"crisp\",\"gem\",\"iconic\",\"undervalued\",\"deal\",\"bargain\",\"goat\",\"legend\",\"heat\"}\n",
        "_NEG = {\"creased\",\"damage\",\"ding\",\"scratches\",\"scratched\",\"off-center\",\"offcenter\",\"trimmed\",\"fake\",\"reprint\",\"altered\",\"stain\",\"worst\",\"overpriced\"}\n",
        "\n",
        "def _sentiment_features(text: str) -> list[Signal]:\n",
        "    tokens = re.findall(r\"[a-zA-Z\\-]+\", text.lower())\n",
        "    pos_hits = sum(1 for w in tokens if w in _POS)\n",
        "    neg_hits = sum(1 for w in tokens if w in _NEG)\n",
        "    total = max(pos_hits + neg_hits, 1)\n",
        "    polarity = (pos_hits - neg_hits) / total      # [-1,1]\n",
        "    val = (polarity + 1) / 2                      # [0,1]\n",
        "    return [Signal(\"sentiment\", val, 0.05, f\"lexicon polarity {polarity:.2f}\")]\n",
        "\n",
        "def _signals_ebay_listing(html: str) -> list[Signal]:\n",
        "    s: list[Signal] = []\n",
        "    text = _cheap_text(html)\n",
        "    lower = text.lower()\n",
        "\n",
        "    # Sentiment (small nudge)\n",
        "    s.extend(_sentiment_features(text))\n",
        "\n",
        "    # Seller feedback %\n",
        "    m = re.search(r\"(\\d{1,3}\\.\\d)\\%\\s*positive feedback\", text, re.I)\n",
        "    if m:\n",
        "        pct = float(m.group(1))\n",
        "        v = 0.2 + 0.8 * (pct / 100.0)            # maps 0..100% → ~0.2..1.0\n",
        "        s.append(Signal(\"seller_feedback_pct\", min(v, 1.0), 0.12, f\"Seller feedback {pct}%\"))\n",
        "    else:\n",
        "        s.append(Signal(\"seller_feedback_pct\", 0.55, 0.06, \"Feedback % not found\"))\n",
        "\n",
        "    # Seller feedback count (log scale → diminishing returns)\n",
        "    m2 = re.search(r\"\\((\\d{2,6})\\)\\s*feedback\", text, re.I)\n",
        "    if m2:\n",
        "        cnt = int(m2.group(1))\n",
        "        v = min(math.log10(max(cnt, 1)) / 5.0 + 0.4, 1.0)\n",
        "        s.append(Signal(\"seller_feedback_count\", v, 0.08, f\"Feedback count {cnt}\"))\n",
        "\n",
        "    # Top Rated badge\n",
        "    if re.search(r\"top rated seller\", text, re.I):\n",
        "        s.append(Signal(\"top_rated\", 1.0, 0.06, \"Top Rated Seller badge\"))\n",
        "\n",
        "    # Returns policy\n",
        "    if re.search(r\"\\b(30|60)\\s*day returns?\\b\", text, re.I):\n",
        "        s.append(Signal(\"returns_policy\", 0.92, 0.05, \"30/60-day returns\"))\n",
        "    elif re.search(r\"no returns\", text, re.I):\n",
        "        s.append(Signal(\"returns_policy\", 0.50, 0.05, \"No returns\"))\n",
        "\n",
        "    # Listing specificity: hobby keywords + jersey number hint\n",
        "    term_score = 0.0\n",
        "    for k, w in CARD_TERMS.items():\n",
        "        if k in lower:\n",
        "            term_score += w\n",
        "    if re.search(r\"\\b#?\\d{1,2}\\b\", lower):\n",
        "        term_score += 0.04\n",
        "    term_score = min(term_score, 1.0)\n",
        "    s.append(Signal(\"card_specificity_terms\", term_score, 0.14, \"Hobby keywords present\"))\n",
        "\n",
        "    # Year + Set present\n",
        "    any_year = bool(re.search(r\"\\b(19|20)\\d{2}\\b\", text))\n",
        "    any_set  = bool(re.search(r\"(prizm|topps|merlin|select|optic|megacracks|chrome|sapphire|logofractor)\", lower))\n",
        "    s.append(Signal(\"year_set_hint\", 1.0 if (any_year and any_set) else 0.6, 0.06, \"Year+Set mentioned\"))\n",
        "\n",
        "    # Images\n",
        "    imgs = _count_images(html)\n",
        "    if imgs >= 8:\n",
        "        s.append(Signal(\"image_count\", 0.95, 0.05, f\"{imgs} images\"))\n",
        "    elif imgs >= 4:\n",
        "        s.append(Signal(\"image_count\", 0.75, 0.05, f\"{imgs} images\"))\n",
        "    else:\n",
        "        s.append(Signal(\"image_count\", 0.55, 0.05, f\"{imgs} images\"))\n",
        "\n",
        "    # Shipping traceability\n",
        "    if re.search(r\"ships from\\s+[A-Za-z ]+\", lower):\n",
        "        s.append(Signal(\"shipping_from\", 0.70, 0.03, \"Ships-from present\"))\n",
        "\n",
        "    return s\n",
        "\n",
        "# -------------------------\n",
        "# Core scorer & batch rank\n",
        "# -------------------------\n",
        "def score_url(\n",
        "    url: str,\n",
        "    *,\n",
        "    dry_run: bool=False,\n",
        "    cohort_scores: t.Sequence[float] | None=None,\n",
        "    session: t.Any | None=None,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Evaluate one URL and return structured credibility JSON (public contract).\n",
        "    \"\"\"\n",
        "    t0 = time.perf_counter()\n",
        "    errors: list[str] = []\n",
        "    signals: list[Signal] = []\n",
        "\n",
        "    # 1) URL parse/validate\n",
        "    try:\n",
        "        parsed = urlparse(url)\n",
        "        if parsed.scheme not in {\"http\",\"https\"} or not parsed.netloc:\n",
        "            raise ValueError(\"URL must include http(s) scheme and host\")\n",
        "        host = parsed.hostname or \"\"\n",
        "    except Exception as e:\n",
        "        result = ScoreResult(\n",
        "            url=url, status=\"invalid_url\", score_abs=0.0, score_pct=None, signals=[],\n",
        "            errors=[f\"invalid_url: {e}\"], meta={\"fetched_at\": _now_iso(), \"elapsed_ms\": _elapsed_ms(t0)}\n",
        "        )\n",
        "        return response_json(result)\n",
        "\n",
        "    # 2) Domain/transport priors\n",
        "    signals.append(_signal_domain_baseline(host))\n",
        "    signals.append(_signal_transport_security(parsed.scheme))\n",
        "\n",
        "    # 3) Fetch or synthesize content\n",
        "    html: str | None = None\n",
        "    status: str = \"ok\"\n",
        "    fetched_ms = None\n",
        "\n",
        "    if dry_run:\n",
        "        html = _synthetic_page_for(host)\n",
        "        fetched_ms = _elapsed_ms(t0)\n",
        "    else:\n",
        "        if requests is None:\n",
        "            errors.append(\"requests_not_available\"); status = \"fetch_error\"\n",
        "        else:\n",
        "            try:\n",
        "                sess = session or requests.Session()\n",
        "                r = sess.get(url, headers=DEFAULT_HEADERS, timeout=DEFAULT_TIMEOUT_S)\n",
        "                fetched_ms = _elapsed_ms(t0)\n",
        "                if r.status_code >= 400:\n",
        "                    raise RuntimeError(f\"HTTP {r.status_code}\")\n",
        "                html = r.text\n",
        "            except Exception as e:\n",
        "                errors.append(f\"fetch_error: {e}\"); status = \"fetch_error\"\n",
        "\n",
        "    # 4) Content/platform signals\n",
        "    if html:\n",
        "        try:\n",
        "            signals.extend(_signals_content_quality(html))\n",
        "        except Exception as e:\n",
        "            errors.append(f\"content_parse_error: {e}\")\n",
        "        try:\n",
        "            if EbayLike.search(host or \"\"):\n",
        "                signals.extend(_signals_ebay_listing(html))\n",
        "        except Exception as e:\n",
        "            errors.append(f\"ebay_parse_error: {e}\")\n",
        "\n",
        "    # 5) Aggregate score\n",
        "    raw = sum(s.contribution() for s in signals)\n",
        "    abs_score = _squash_0_100(raw)\n",
        "\n",
        "    # 6) Optional percentile vs cohort\n",
        "    pct = None\n",
        "    if cohort_scores:\n",
        "        try:\n",
        "            pct = _percentile(abs_score, list(cohort_scores))\n",
        "        except Exception as e:\n",
        "            errors.append(f\"percentile_error: {e}\")\n",
        "\n",
        "    # 7) Assemble payload\n",
        "    result = ScoreResult(\n",
        "        url=url,\n",
        "        status=status,\n",
        "        score_abs=abs_score,\n",
        "        score_pct=pct,\n",
        "        signals=signals,\n",
        "        errors=errors,\n",
        "        meta={\n",
        "            \"host\": host,\n",
        "            \"is_ebay\": bool(EbayLike.search(host or \"\")),\n",
        "            \"fetched_at\": _now_iso(),\n",
        "            \"elapsed_ms\": _elapsed_ms(t0),\n",
        "            \"fetch_ms\": fetched_ms,\n",
        "            \"version\": \"d3-0.2\",\n",
        "        },\n",
        "    )\n",
        "    return response_json(result)\n",
        "\n",
        "def rank_listings(urls: list[str], *, dry_run: bool=False) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Score a list of URLs, attach within-batch percentiles, return rows sorted by absolute score desc.\n",
        "    \"\"\"\n",
        "    rows: list[dict] = []\n",
        "    sess = None if dry_run else (requests.Session() if requests else None)\n",
        "\n",
        "    tmp: list[dict] = []\n",
        "    abs_scores: list[float] = []\n",
        "    for u in urls:\n",
        "        r = score_url(u, dry_run=dry_run, session=sess)\n",
        "        tmp.append(r)\n",
        "        abs_scores.append(r[\"score\"][\"absolute\"])\n",
        "\n",
        "    for r in tmp:\n",
        "        r[\"score\"][\"percentile\"] = _percentile(r[\"score\"][\"absolute\"], abs_scores)\n",
        "        rows.append(r)\n",
        "\n",
        "    rows.sort(key=lambda d: d[\"score\"][\"absolute\"], reverse=True)\n",
        "    return rows\n",
        "\n",
        "def to_dataframe(rows: list[dict]):\n",
        "    \"\"\"\n",
        "    Convenience: flatten results into a DataFrame; expands signal contributions as columns.\n",
        "    \"\"\"\n",
        "    if pd is None:\n",
        "        raise RuntimeError(\"pandas not installed\")\n",
        "    flat = []\n",
        "    for r in rows:\n",
        "        base = {\n",
        "            \"url\": r[\"url\"],\n",
        "            \"score_abs\": r[\"score\"][\"absolute\"],\n",
        "            \"score_pct\": r[\"score\"].get(\"percentile\"),\n",
        "            \"status\": r[\"status\"],\n",
        "            \"host\": r[\"meta\"].get(\"host\"),\n",
        "            \"is_ebay\": r[\"meta\"].get(\"is_ebay\"),\n",
        "        }\n",
        "        for sig in r[\"signals\"]:\n",
        "            base[f\"sig_{sig['name']}\"] = sig[\"value\"] * sig[\"weight\"]\n",
        "        flat.append(base)\n",
        "    return pd.DataFrame(flat).sort_values(\"score_abs\", ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la src | head -n 20\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRrLxdyrY0HD",
        "outputId": "0656f729-64f6-4665-dd97-c32a524abcde"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 28\n",
            "drwxr-xr-x 2 root root  4096 Oct  3 18:43 .\n",
            "drwxr-xr-x 1 root root  4096 Oct  3 18:43 ..\n",
            "-rw-r--r-- 1 root root 18380 Oct  3 18:43 scorer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.scorer import score_url, rank_listings\n",
        "rows = rank_listings([\"https://www.ebay.com/itm/123\",\"https://www.comc.com/Cards/Soccer\"], dry_run=True)\n",
        "rows[0][\"score\"], rows[0][\"meta\"][\"host\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRssPlIvY1p3",
        "outputId": "89cd2363-6128-4451-87d1-24f3f597c24c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'absolute': 41.93, 'percentile': 100.0}, 'www.ebay.com')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/pplx_client.py\n",
        "# pplx_client.py\n",
        "# Thin client for Perplexity Chat Completions API to discover URLs.\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, re, json, time\n",
        "import typing as t\n",
        "import requests\n",
        "\n",
        "def _extract_urls(text: str) -> list[str]:\n",
        "    urls = re.findall(r'https?://[^\\s)>\\]\"}]+', text, flags=re.I)\n",
        "    out, seen = [], set()\n",
        "    for u in urls:\n",
        "        u = u.rstrip('.,);:')\n",
        "        if u not in seen:\n",
        "            seen.add(u); out.append(u)\n",
        "    return out\n",
        "\n",
        "def pplx_search_sources(player: str, *, max_urls: int = 12, api_key: str | None = None, model: str = \"sonar-pro\") -> dict:\n",
        "    \"\"\"\n",
        "    Returns: {\"prompt\": <str>, \"answer\": <str>, \"citations\": [url,...], \"urls\": [url,...]}\n",
        "    \"\"\"\n",
        "    key = api_key or os.getenv(\"PERPLEXITY_API_KEY\")\n",
        "    assert key, \"Missing PERPLEXITY_API_KEY (env var). In Colab, set it from Secrets.\"\n",
        "\n",
        "    base = \"https://api.perplexity.ai/chat/completions\"\n",
        "    headers = {\"Authorization\": f\"Bearer {key}\", \"Content-Type\": \"application/json\"}\n",
        "\n",
        "    system = (\n",
        "        \"You are a research assistant. Return reputable URLs that directly reference \"\n",
        "        \"specific soccer trading cards (set, year, variant, grade/serial where possible). \"\n",
        "        \"Prefer official marketplaces (eBay item pages, PWCC, Goldin), manufacturer pages, \"\n",
        "        \"and credible hobby references. Include recent/active listings where possible.\"\n",
        "    )\n",
        "    user = (\n",
        "        f\"Player: {player}\\n\"\n",
        "        \"Task: Find specific active or recent listings and authoritative references for this player's cards. \"\n",
        "        \"Return direct item or reference URLs (not just homepages). Include a mix of marketplaces and credible sources.\"\n",
        "    )\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ],\n",
        "        \"temperature\": 0.2,\n",
        "        \"top_p\": 0.9\n",
        "    }\n",
        "    r = requests.post(base, headers=headers, data=json.dumps(payload), timeout=30)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "\n",
        "    answer = \"\"\n",
        "    try:\n",
        "        answer = data[\"choices\"][0][\"message\"][\"content\"]\n",
        "    except Exception:\n",
        "        answer = json.dumps(data)[:2000]\n",
        "\n",
        "    citations = []\n",
        "    try:\n",
        "        citations = data.get(\"citations\") or data[\"choices\"][0][\"message\"].get(\"citations\") or []\n",
        "    except Exception:\n",
        "        citations = []\n",
        "\n",
        "    urls = list(dict.fromkeys((citations or []) + _extract_urls(answer)))[:max_urls]\n",
        "    return {\"prompt\": user, \"answer\": answer, \"citations\": citations, \"urls\": urls}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx3Ldk9baOOx",
        "outputId": "e5184604-981e-4226-953d-64a8c3011eed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/pplx_client.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# app.py — Streamlit front-end for Deliverable 3\n",
        "import os\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "from src.pplx_client import pplx_search_sources\n",
        "from src.scorer import rank_listings\n",
        "\n",
        "st.set_page_config(page_title=\"Soccer Card Source Credibility\", layout=\"wide\")\n",
        "st.title(\"Soccer Card Source Credibility (RAG + Scorer)\")\n",
        "st.caption(\"Enter a player; we’ll fetch recent sources via Perplexity, then score each URL’s credibility.\")\n",
        "\n",
        "# In Colab, you can set this env var at runtime from Secrets (see notebook cell):\n",
        "# os.environ['PERPLEXITY_API_KEY'] = '...'  # (handled in notebook)\n",
        "\n",
        "player = st.text_input(\"Player name\", value=\"Bukayo Saka\")\n",
        "col_a, col_b, col_c = st.columns([1,1,2])\n",
        "with col_a:\n",
        "    max_urls = st.slider(\"Max URLs\", min_value=5, max_value=25, value=12, step=1)\n",
        "with col_b:\n",
        "    dry_run = st.checkbox(\"Dry run (synthetic pages for scoring)\", value=False)\n",
        "with col_c:\n",
        "    st.info(\"Scores are 0–100 with percentiles within this cohort. Click a row for rationale in the URL.\")\n",
        "\n",
        "if st.button(\"Search & Score\") and player.strip():\n",
        "    with st.spinner(\"Searching Perplexity and scoring sources...\"):\n",
        "        discovery = pplx_search_sources(player.strip(), max_urls=max_urls)\n",
        "        urls = discovery[\"urls\"]\n",
        "        if not urls:\n",
        "            st.warning(\"No URLs found. Try a different player spelling or a well-known card.\")\n",
        "        else:\n",
        "            rows = rank_listings(urls, dry_run=dry_run)\n",
        "\n",
        "            # Build table with top signal rationales (compact)\n",
        "            table = []\n",
        "            for r in rows:\n",
        "                top3 = sorted(r[\"signals\"], key=lambda s: s[\"value\"]*s[\"weight\"], reverse=True)[:3]\n",
        "                rationale = \"; \".join([f\"{s['name']}: {s['rationale']}\" for s in top3])\n",
        "                table.append({\n",
        "                    \"Score\": r[\"score\"][\"absolute\"],\n",
        "                    \"Pct\": r[\"score\"][\"percentile\"],\n",
        "                    \"Host\": r[\"meta\"][\"host\"],\n",
        "                    \"Status\": r[\"status\"],\n",
        "                    \"URL\": r[\"url\"],\n",
        "                    \"Top rationales\": rationale\n",
        "                })\n",
        "            df = pd.DataFrame(table).sort_values(\"Score\", ascending=False)\n",
        "            st.dataframe(df, use_container_width=True)\n",
        "            st.caption(f\"Perplexity returned {len(urls)} URLs for {player.strip()}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgo4wcePbXb_",
        "outputId": "6a4dbc05-3e68-4863-f400-9f43bba7738e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "streamlit\n",
        "requests\n",
        "beautifulsoup4\n",
        "lxml\n",
        "scikit-learn\n",
        "pandas\n",
        "numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q7McAGRcI5V",
        "outputId": "035942e0-d03f-4bae-8fd3-2b8033a23c6a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    }
  ]
}